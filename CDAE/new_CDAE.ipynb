{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/larry/Py3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from CDAE import AutoEncoder\n",
    "from tqdm import trange\n",
    "from utils import *\n",
    "import clustering\n",
    "\n",
    "from sklearn.cluster import KMeans, spectral_clustering\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity of ratings is 10.49%\n",
      "num. of users: 839, num. of items: 99\n",
      "8709\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/class/rating_data.csv')\n",
    "df['freq'] = df.groupby('uid')['uid'].transform('count')  # count frequncy by column's values\n",
    "df = df[df['freq'] > 5]  # remove row which corresponding frequence < 5\n",
    "\n",
    "userList = df['uid'].unique()\n",
    "itemList = df['iid'].unique()\n",
    "\n",
    "total_usr = len(df['uid'].unique())\n",
    "total_item = len(df['iid'].unique())\n",
    "\n",
    "sparsity = len(df)/(total_usr*total_item)\n",
    "print(\"sparsity of ratings is %.2f%%\" %(sparsity*100))\n",
    "print (\"num. of users: %d, num. of items: %d\" % (total_usr, total_item))\n",
    "print (len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def get_map(list_):\n",
    "    map_ = {}\n",
    "    for idx, ident in enumerate(list_):\n",
    "        map_[ident] = idx\n",
    "        \n",
    "    return map_\n",
    "\n",
    "def get_matrix(data):\n",
    "    matrix = np.zeros((total_usr, total_item), dtype=np.float32)\n",
    "    for line in data:\n",
    "        uid = user_map[line[0]]\n",
    "        iid = item_map[line[1]]\n",
    "        matrix[uid, iid] = 1\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def train_test_split(df, time_interval, split_rate=0.5):\n",
    "    start_time = min(df['timestamp'])\n",
    "    end_time = max(df['timestamp'])\n",
    "    time_elapse = (end_time-start_time) // time_interval\n",
    "    split_time = start_time + math.floor(time_elapse * (1-split_rate)) * time_interval\n",
    "    \n",
    "    while split_time < end_time:\n",
    "        df_train = df[df['timestamp'] < split_time]\n",
    "        df_train = df_train[df_train['timestamp'] >= split_time - 6 * time_interval]\n",
    "        \n",
    "        df_test_1 = df[df['timestamp'] >= split_time - 3*time_interval]\n",
    "        df_test_1 = df_test_1[df_test_1['timestamp'] < split_time]\n",
    "        \n",
    "        df_test_2 = df[df['timestamp'] >= split_time]\n",
    "        df_test_2 = df_test_2[df_test_2['timestamp'] < split_time + time_interval]\n",
    "        \n",
    "        # start_time += time_interval\n",
    "        # split_time = start_time + math.floor(time_elapse * (1-split_rate)) * time_interval\n",
    "        split_time += time_interval\n",
    "        \n",
    "        yield df_train, df_test_1, df_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_map = get_map(userList)\n",
    "item_map = get_map(itemList)\n",
    "    \n",
    "user_time_interval = 3 * 30 * 24 * 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator = train_test_split(df, user_time_interval)\n",
    "\n",
    "NUM_CLUSTER = 10\n",
    "top_items_list_75 = []\n",
    "top_gt_list_next = []\n",
    "top_gt_list_now = []\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        df_train, df_test_now, df_test_next = next(generator)\n",
    "\n",
    "        train_data = df_train.as_matrix()\n",
    "        test_data_now = df_test_now.as_matrix()\n",
    "        test_data_next = df_test_next.as_matrix()\n",
    "\n",
    "        user_train_matrix = get_matrix(train_data)\n",
    "        user_test_matrix_now = get_matrix(test_data_now)\n",
    "        user_test_matrix_next = get_matrix(test_data_next)\n",
    "        \n",
    "        train_user = np.nonzero(np.count_nonzero(user_train_matrix, axis=1))[0]\n",
    "        test_user_now = np.nonzero(np.count_nonzero(user_test_matrix_now, axis=1))[0]\n",
    "        \n",
    "        top_n = np.count_nonzero(user_train_matrix, axis=0).argsort()[::-1][:30]\n",
    "        others = [k for k in range(total_item) if k not in top_n]\n",
    "\n",
    "        # Train at first to get user_vector\n",
    "        tf.reset_default_graph()\n",
    "        autoencoder = AutoEncoder(user_num=total_usr, item_num=total_item, mode='user', \n",
    "                                  denoise_function=None, loss_function='cross_entropy',\n",
    "                                  denoising=False, batch_size=1, epochs=200)\n",
    "\n",
    "        autoencoder.train_all(rating=user_train_matrix, train_idents=train_user, topN=top_n, weight=30)\n",
    "\n",
    "        autoencoder.model_save(1)\n",
    "        \n",
    "        # Get specify vectors/feature vectors\n",
    "        vector_matrices = autoencoder.sess.run(autoencoder.vector_matrix)\n",
    "        \"\"\"vector_matrices = autoencoder.sess.run(\n",
    "            autoencoder.code,\n",
    "            feed_dict={\n",
    "                autoencoder.input: user_train_matrix,\n",
    "                autoencoder.ident: [x for x in range(total_usr)]\n",
    "            })\"\"\"\n",
    "        exist_vectors = np.take(vector_matrices, train_user, axis=0)\n",
    "        \n",
    "        # Clustering\n",
    "        pca_out = clustering.get_pca_out(exist_vectors)\n",
    "        kmeans = clustering.calculate_kmeans(pca_out, NUM_CLUSTER=NUM_CLUSTER)\n",
    "        \n",
    "        label_index, label_count = clustering.get_cluster_attributes(kmeans, NUM_CLUSTER=NUM_CLUSTER)\n",
    "        \n",
    "        # gether input data\n",
    "        data = {\n",
    "            'TRAIN_MATRIX': user_train_matrix,\n",
    "            'TEST_MATRIX_NOW': user_test_matrix_now,\n",
    "            'TEST_MATRIX_NEXT': user_test_matrix_next,\n",
    "            'TRAIN_USER': train_user,\n",
    "            'TEST_USER_NOW': test_user_now,\n",
    "            'LABEL_INDEX': label_index,\n",
    "        }\n",
    "        \n",
    "        # calculate top_N for each cluster\n",
    "        cluster_top = clustering.calculate_cluster_top(\n",
    "            data,\n",
    "            total_usr,\n",
    "            total_item,\n",
    "            NUM_CLUSTER=NUM_CLUSTER,\n",
    "            batch_size=1,\n",
    "            weight=30,\n",
    "            denoise_function=None,\n",
    "            loss_function='cross_entropy')\n",
    "        \n",
    "        # get cluster's item distribution\n",
    "        cluster_distribution = clustering.get_distribution(data, NUM_CLUSTER=NUM_CLUSTER)\n",
    "        \n",
    "        # calculate score for top items\n",
    "        score_map = clustering.count_score(cluster_top, label_count, len(test_user_now), cluster_distribution, exponent=1.0001, alpha=100)\n",
    "        \n",
    "        # get top items\n",
    "        top_N = clustering.get_score_top(score_map, N=30)\n",
    "        \n",
    "        # gether predict top items\n",
    "        top_items_list_75.append(top_N)\n",
    "        \n",
    "        # gether ground truth items\n",
    "        ground_truth_next = np.count_nonzero(user_test_matrix_next, axis=0).argsort()[::-1][:30]\n",
    "        ground_truth_now = np.count_nonzero(user_test_matrix_now, axis=0).argsort()[::-1][:30]\n",
    "        top_gt_list_next.append(ground_truth_next)\n",
    "        top_gt_list_now.append(ground_truth_now)\n",
    "                \n",
    "    except StopIteration:\n",
    "        break\n",
    "        \n",
    "\n",
    "top_items_list_75 = np.asarray(top_items_list_75)\n",
    "top_gt_list_now = np.asarray(top_gt_list_now)\n",
    "top_gt_list_next = np.asarray(top_gt_list_next)\n",
    "\n",
    "np.save('./rec_lists/class_clustering_rec_lists.npy', top_items_list_75)\n",
    "np.save('./rec_lists/class_gt_now.npy', top_gt_list_now)\n",
    "np.save('./rec_lists/class_gt_next.npy', top_gt_list_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hit_ratio_top_30 = []\n",
    "hit_ratio_top_10 = []\n",
    "hit_ratio_top_5 = []\n",
    "f1_top_30 = []\n",
    "f1_top_10 = []\n",
    "f1_top_5 = []\n",
    "\n",
    "for i, j in zip(top_items_list_75, top_gt_list_next):\n",
    "    hit_ratio_top_30.append(hit_recall(i, j, N=30))\n",
    "    hit_ratio_top_10.append(hit_recall(i, j, N=10))\n",
    "    hit_ratio_top_5.append(hit_recall(i, j, N=5))\n",
    "    \n",
    "    k = np.asarray([i])\n",
    "    q = np.asarray([j])\n",
    "    f1_top_30.append(2*hit_recall(i, j, N=30)**2/(2*hit_recall(i, j, N=30)))\n",
    "    f1_top_10.append(2*hit_recall(i, j, N=10)**2/(2*hit_recall(i, j, N=10)))\n",
    "    try:\n",
    "        f1_top_5.append(2*hit_recall(i, j, N=5)**2/(2*hit_recall(i, j, N=5)))\n",
    "    except ZeroDivisionError:\n",
    "        f1_top_5.append(None)\n",
    "    \n",
    "\n",
    "hit_ratio_still_top30 = []\n",
    "hit_ratio_still_top10 = []\n",
    "hit_ratio_still_top5 = []\n",
    "f1_still_30 = []\n",
    "f1_still_10 = []\n",
    "f1_still_5 = []\n",
    "\n",
    "for i, j ,k in zip(top_items_list_75, top_gt_list_now, top_gt_list_next):\n",
    "    still_in_items_30 = []\n",
    "    still_in_items_10 = []\n",
    "    still_in_items_5 = []\n",
    "    \n",
    "    for q in k:\n",
    "        if q in j:\n",
    "            still_in_items_30.append(q)\n",
    "            \n",
    "    for q in k[:10]:\n",
    "        if q in j[:10]:\n",
    "            still_in_items_10.append(q)\n",
    "            \n",
    "    for q in k[:5]:\n",
    "        if q in j[:5]:\n",
    "            still_in_items_5.append(q)\n",
    "            \n",
    "    hit_ratio_still_top30.append(hit_recall(i, still_in_items_30, N=30))\n",
    "    hit_ratio_still_top10.append(hit_recall(i, still_in_items_10, N=10))\n",
    "    hit_ratio_still_top5.append(hit_recall(i, still_in_items_5, N=5))\n",
    "    \n",
    "    x = np.asarray([i])\n",
    "    y_30 = np.asarray([still_in_items_30])\n",
    "    y_10 = np.asarray([still_in_items_10])\n",
    "    y_5 = np.asarray([still_in_items_5])\n",
    "    \n",
    "    f1_still_30.append(2 * hit_recall(i, still_in_items_30, N=30) * \\\n",
    "                       (hit_recall(i, still_in_items_30, N=30)*min(30, len(still_in_items_30))/30) / \\\n",
    "                       (hit_recall(i, still_in_items_30, N=30) + \\\n",
    "                       (hit_recall(i, still_in_items_30, N=30)*min(30, len(still_in_items_30))/30)))\n",
    "    f1_still_10.append(2 * hit_recall(i, still_in_items_10, N=10) * \\\n",
    "                       (hit_recall(i, still_in_items_10, N=10)*min(10, len(still_in_items_10))/10) / \\\n",
    "                       (hit_recall(i, still_in_items_10, N=10) + \\\n",
    "                       (hit_recall(i, still_in_items_10, N=10)*min(10, len(still_in_items_10))/10)))\n",
    "    try:\n",
    "        f1_still_5.append(2 * hit_recall(i, still_in_items_5, N=5) * \\\n",
    "                           (hit_recall(i, still_in_items_5, N=5)*min(5, len(still_in_items_5))/5) / \\\n",
    "                           (hit_recall(i, still_in_items_5, N=5) + \\\n",
    "                           (hit_recall(i, still_in_items_5, N=5)*min(5, len(still_in_items_5))/5)))\n",
    "    except:\n",
    "        f1_still_5.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(hit_ratio_top_30)), hit_ratio_top_30, color='blue', label='hit_ratios')\n",
    "plt.legend(loc=\"upper right\")\n",
    "# plt.title(\"Time: %d, Item: %d, mean: %f\" % (i, top_items_list[i][j], top_means_list[i][j][1]))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('ratio')\n",
    "plt.title('NOW')\n",
    "plt.show()\n",
    "plt.gcf().clear()\n",
    "print (\"Hit ratio top 30: %f\" % (sum(hit_ratio_top_30[:-1])/len(hit_ratio_top_30[:-1])))\n",
    "print (\"Hit ratio top 10: %f\" % (sum(hit_ratio_top_10[:-1])/len(hit_ratio_top_10[:-1])))\n",
    "print (\"Hit ratio top 5: %f\" % (sum(hit_ratio_top_5[:-1])/len(hit_ratio_top_5[:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(hit_ratio_still_top10)), hit_ratio_still_top10, color='blue', label='hit_ratios')\n",
    "plt.legend(loc=\"upper right\")\n",
    "# plt.title(\"Time: %d, Item: %d, mean: %f\" % (i, top_items_list[i][j], top_means_list[i][j][1]))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('ratio')\n",
    "plt.title('NEXT')\n",
    "plt.show()\n",
    "plt.gcf().clear()\n",
    "print (\"Still in hit ratio top 30: %f\" % (sum(hit_ratio_still_top30[:-1])/len(hit_ratio_still_top30[:-1])))\n",
    "print (\"Still in hit ratio top 10: %f\" % (sum(hit_ratio_still_top10[:-1])/len(hit_ratio_still_top10[:-1])))\n",
    "print (\"Still in hit ratio top 5: %f\" % (sum(hit_ratio_still_top5[:-1])/len(hit_ratio_still_top5[:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"F1 score top 30: %f\" % (sum(f1_top_30[:-1])/len(f1_top_30[:-1])))\n",
    "print (\"F1 score top 10: %f\" % (sum(f1_top_10[:-1])/len(f1_top_10[:-1])))\n",
    "print (\"F1 score top 5: %f\" % (sum(f1_top_5[:-1])/len(f1_top_5[:-1])))\n",
    "print ()\n",
    "print (\"F1 score still top 30: %f\" % (sum(f1_still_30[:-1])/len(f1_still_30[:-1])))\n",
    "print (\"F1 score still top 10: %f\" % (sum(f1_still_10[:-1])/len(f1_still_10[:-1])))\n",
    "print (\"F1 score still top 5: %f\" % (sum(f1_still_5[:-1])/len(f1_still_5[:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hit_ratio_not_top30 = []\n",
    "hit_ratio_not_top10 = []\n",
    "hit_ratio_not_top5 = []\n",
    "\n",
    "for i, j ,k in zip(top_items_list_75, top_gt_list_now, top_gt_list_next):\n",
    "    not_in_items_30 = []\n",
    "    not_in_items_10 = []\n",
    "    not_in_items_5 = []\n",
    "    \n",
    "    for q in j:\n",
    "        if q not in k:\n",
    "            not_in_items_30.append(q)\n",
    "            \n",
    "    for q in j[:10]:\n",
    "        if q not in k[:10]:\n",
    "            not_in_items_10.append(q)\n",
    "            \n",
    "    for q in j[:5]:\n",
    "        if q not in k[:5]:\n",
    "            not_in_items_5.append(q)\n",
    "            \n",
    "    hit_ratio_not_top30.append(1-hit_recall(i, not_in_items_30, N=30))\n",
    "    hit_ratio_not_top10.append(1-hit_recall(i, not_in_items_10, N=10))\n",
    "    hit_ratio_not_top5.append(1-hit_recall(i, not_in_items_5, N=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Not in hit ratio top 30: %f\" % (sum(hit_ratio_not_top30[:-1])/len(hit_ratio_not_top30[:-1])))\n",
    "print (\"Not in hit ratio top 10: %f\" % (sum(hit_ratio_not_top10[:-1])/len(hit_ratio_not_top10[:-1])))\n",
    "print (\"Not in hit ratio top 5: %f\" % (sum(hit_ratio_not_top5[:-1])/len(hit_ratio_not_top5[:-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Still in ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generator = train_test_split(df, user_time_interval, split_rate=0.5)\n",
    "count = 0\n",
    "still_in_ratio = []\n",
    "watched_people = []\n",
    "watched_count = []\n",
    "watched_item = []\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        df_train, df_test_1, df_test_2 = next(generator)\n",
    "        \n",
    "        test_data_1 = df_test_1.as_matrix()\n",
    "        test_data_2 = df_test_2.as_matrix()\n",
    "        user_test_matrix_1 = get_matrix(test_data_1)\n",
    "        user_test_matrix_2 = get_matrix(test_data_2)\n",
    "        \n",
    "\n",
    "        ground_truth_next = np.count_nonzero(user_test_matrix_2, axis=0).argsort()[::-1][:30]\n",
    "        \n",
    "        watched_count.append(sum(np.count_nonzero(user_test_matrix_1, axis=0))/len(np.count_nonzero(user_test_matrix_1, axis=0)))\n",
    "        watched_item.append(len(np.nonzero(np.count_nonzero(user_test_matrix_1, axis=0))[0]))\n",
    "        watched_people.append(len(np.nonzero(np.count_nonzero(user_test_matrix_1, axis=1))[0]))\n",
    "        \n",
    "        ground_truth_now = np.count_nonzero(user_test_matrix_1, axis=0).argsort()[::-1][:30]\n",
    "        \n",
    "        \n",
    "        still_in = hit_recall(ground_truth_next, ground_truth_now, N=30)\n",
    "        \n",
    "        still_in_ratio.append(still_in)\n",
    "        count += 1\n",
    "        print (count)\n",
    "        \n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(still_in_ratio)), still_in_ratio, color='blue', label='still_in_ratio')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Netflix_top_30_still_in_ratio\")\n",
    "plt.xlabel('Users')\n",
    "plt.ylabel('score')\n",
    "plt.savefig(\"./prediction_weekly/Netflix_still_in_top_30.jpg\")\n",
    "plt.show()\n",
    "plt.gcf().clear()\n",
    "\n",
    "print (sum(still_in_ratio)/len(still_in_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:28<00:00,  7.07it/s]\n",
      "100%|██████████| 200/200 [00:36<00:00,  5.54it/s]\n",
      "100%|██████████| 200/200 [00:43<00:00,  4.62it/s]\n",
      "100%|██████████| 200/200 [00:48<00:00,  4.16it/s]\n",
      "100%|██████████| 200/200 [00:55<00:00,  3.60it/s]\n",
      "100%|██████████| 200/200 [01:08<00:00,  2.93it/s]\n",
      "100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "100%|██████████| 200/200 [01:33<00:00,  2.13it/s]\n",
      "100%|██████████| 200/200 [01:39<00:00,  2.00it/s]\n",
      "100%|██████████| 200/200 [01:48<00:00,  1.85it/s]\n",
      "100%|██████████| 200/200 [01:54<00:00,  1.75it/s]\n",
      "100%|██████████| 200/200 [02:00<00:00,  1.67it/s]\n",
      "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "generator = train_test_split(df, user_time_interval)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "autoencoder = AutoEncoder(user_num=total_usr, item_num=total_item, mode='user', loss_function='log_loss',\n",
    "                          denoise_function='dropout', denoising=False, batch_size=1, epochs=200)\n",
    "\n",
    "test_out_top = []\n",
    "top_items_list_all = []\n",
    "top_items_list_75 = []\n",
    "top_items_list_85 = []\n",
    "top_means_list = []\n",
    "top_gt_list_next = []\n",
    "top_gt_list_now = []\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        df_train, df_test_1, df_test_2 = next(generator)\n",
    "\n",
    "        train_data = df_train.as_matrix()\n",
    "        test_data_1 = df_test_1.as_matrix()\n",
    "        test_data_2 = df_test_2.as_matrix()\n",
    "\n",
    "        user_train_matrix = get_matrix(train_data)\n",
    "        user_test_matrix_1 = get_matrix(test_data_1)\n",
    "        user_test_matrix_2 = get_matrix(test_data_2)\n",
    "        \n",
    "        top_n = np.count_nonzero(user_train_matrix, axis=0).argsort()[::-1][:30]\n",
    "        others = [k for k in range(total_item) if k not in top_n]\n",
    "\n",
    "        train_user = np.nonzero(np.count_nonzero(user_train_matrix, axis=1))[0]\n",
    "        test_user_1 = np.nonzero(np.count_nonzero(user_test_matrix_1, axis=1))[0]\n",
    "\n",
    "        autoencoder.train_all(rating=user_train_matrix, train_idents=train_user, topN=None, weight=None)\n",
    "\n",
    "        test_out = autoencoder.predict(user_test_matrix_1, test_user_1)\n",
    "        \n",
    "        \"\"\" out data process \"\"\"\n",
    "        # all data mean\n",
    "        test_out_stat_all = np.mean(test_out, axis=0)\n",
    "        \n",
    "        # upper quartile mean\n",
    "        test_out_stat_75 = []\n",
    "        quartile_75 = np.percentile(test_out, 75, axis=0)\n",
    "        for i in range(test_out.shape[1]):\n",
    "            test_out_stat_75.append(np.mean([x for x in test_out.T[i] if x > quartile_75[i]]))\n",
    "        test_out_stat_75 = np.asarray(test_out_stat_75)\n",
    "        \n",
    "        test_out_stat_85 = []\n",
    "        quartile_85 = np.percentile(test_out, 85, axis=0)\n",
    "        for i in range(test_out.shape[1]):\n",
    "            test_out_stat_85.append(np.mean([x for x in test_out.T[i] if x > quartile_85[i]]))\n",
    "        test_out_stat_85 = np.asarray(test_out_stat_85)\n",
    "        \n",
    "        \"\"\"Get top 10\"\"\"\n",
    "        test_out_rank_all = test_out_stat_all.argsort()[::-1][:30]\n",
    "        test_out_rank_75 = test_out_stat_75.argsort()[::-1][:30]\n",
    "        test_out_rank_85 = test_out_stat_85.argsort()[::-1][:30]\n",
    "        ground_truth_next = np.count_nonzero(user_test_matrix_2, axis=0).argsort()[::-1][:30]\n",
    "        ground_truth_now = np.count_nonzero(user_test_matrix_1, axis=0).argsort()[::-1][:30]\n",
    "        \n",
    "        \"\"\"out data collect\"\"\"\n",
    "        # top_out = np.take(test_out, test_out_rank_all, axis=1).T\n",
    "        # test_out_top.append(top_out)\n",
    "        \n",
    "        top_items_list_all.append(test_out_rank_all)\n",
    "        top_items_list_75.append(test_out_rank_75)\n",
    "        top_items_list_85.append(test_out_rank_85)\n",
    "        top_gt_list_next.append(ground_truth_next)\n",
    "        top_gt_list_now.append(ground_truth_now)\n",
    "        \n",
    "        \"\"\"means = []\n",
    "        for i in test_out_rank_all:\n",
    "            means.append((i, test_out_stat_all[i]))\n",
    "        top_means_list.append(means)\"\"\"\n",
    "        \n",
    "    except StopIteration:\n",
    "        break\n",
    "        \n",
    "# top_items_list_all = np.asarray(top_items_list_all)\n",
    "top_items_list_75 = np.asarray(top_items_list_75)\n",
    "top_items_list_85 = np.asarray(top_items_list_85)\n",
    "top_means_list = np.asarray(top_means_list)\n",
    "top_gt_list_now = np.asarray(top_gt_list_now)\n",
    "top_gt_list_next = np.asarray(top_gt_list_next)\n",
    "\n",
    "np.save('./rec_lists/itri_org_rec_lists.npy', top_items_list_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.38965526  0.05022873  0.59063607  0.04710175  0.47774079  0.21033356\n",
      "  0.01748728  0.3823854   0.03187207  0.05401026  0.02712947  0.08351601\n",
      "  0.01007607  0.03252231  0.03898494  0.18826996  0.1525865   0.38864198\n",
      "  0.06566289  0.02996645  0.04550518  0.02160897  0.08674006  0.13260762\n",
      "  0.0982428   0.21181668  0.08661996  0.52284533  0.0598496   0.799788\n",
      "  0.02612696  0.14254397  0.26623634  0.01706529  0.08340617  0.11883468\n",
      "  0.07308783  0.03995959  0.04486014  0.03757913  0.0176826   0.02552888\n",
      "  0.02324141  0.28580835  0.08427937  0.03931291  0.18164438  0.22499211\n",
      "  0.04502813  0.04803188  0.01379419  0.26758412  0.03275115  0.03697237\n",
      "  0.10946263  0.19010942  0.24229836  0.7821936   0.02660504  0.02828271\n",
      "  0.01920071  0.043816    0.06193104  0.01652925  0.03460154  0.01573307\n",
      "  0.04040642  0.18195875  0.11608242  0.03440028  0.13441916  0.02430603\n",
      "  0.12760074  0.07817332  0.02653882  0.26281956  0.43955365  0.08969804\n",
      "  0.24037383  0.04641305  0.09413739  0.04402523  0.35620216  0.1196714\n",
      "  0.01296471  0.01594265  0.02039562  0.09419745  0.07404155  0.09022991\n",
      "  0.25293672  0.13683635  0.03895467  0.04630017  0.0457473   0.07370802\n",
      "  0.27885056  0.02353993  0.01986851]\n"
     ]
    }
   ],
   "source": [
    "print (test_out_stat_85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12\t13\t14\t15\t16\t17\t18\t19\t20\t21\t22\t23\t24\t25\t26\t27\t28\t29\t30\t31\t32\t33\t34\t35\t36\t37\t38\t39\t40\t41\t42\t43\t44\t45\t46\t47\t48\t49\t50\t51\t52\t53\t54\t55\t56\t57\t58\t59\t60\t61\t62\t63\t64\t65\t66\t67\t68\t69\t70\t71\t72\t73\t74\t75\t76\t77\t78\t79\t80\t81\t82\t83\t84\t85\t86\t87\t88\t89\t90\t91\t92\t93\t94\t95\t96\t97\t98\t99\t"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(test_out.shape[1]):\n",
    "    count += 1\n",
    "    print (count, end=\"\\t\")\n",
    "    mean = [sum(test_out.T[i])/len(test_out.T[i]) for x in range(test_out.shape[0])]\n",
    "    up_85 = [test_out_stat_85[i] for x in range(test_out.shape[0])]\n",
    "    up_75 = [test_out_stat_75[i] for x in range(test_out.shape[0])]\n",
    "    \n",
    "    colors = np.random.rand(test_out.shape[0])\n",
    "    plt.scatter(range(test_out.shape[0]), test_out.T[i], c=colors, alpha=0.5)\n",
    "    plt.plot(range(test_out.shape[0]), mean, 'r--', color='red', label='mean')\n",
    "    plt.plot(range(test_out.shape[0]), up_85, 'r--', color='green', label='up 15')\n",
    "    plt.plot(range(test_out.shape[0]), up_75, 'r--', color='blue', label='up 25')\n",
    "    plt.xlabel('User')\n",
    "    plt.ylabel('score')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title('Video %d prediction scores' % (i))\n",
    "    plt.savefig('./figs/Video_%d_scores' % (i))\n",
    "    plt.gcf().clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for i in trange(top_means_list.shape[0]):\n",
    "    for j in range(top_means_list.shape[1]):\n",
    "        if top_items_list_all[i][j] in top_gt_list_now[i]:\n",
    "            plt.plot(range(len(test_out_top[i][j])), test_out_top[i][j], color='green', label='pred. scores')\n",
    "            plt.hlines(top_means_list[i][j][1], -5, len(test_out_top[i][j])+5, linestyles='solid', color='blue')\n",
    "            plt.legend(loc=\"upper right\")\n",
    "            plt.title(\"Time: %d, Item: %d, mean: %f\" % (i, top_items_list_all[i][j], top_means_list[i][j][1]))\n",
    "            plt.xlabel('Users')\n",
    "            plt.ylabel('score')\n",
    "            plt.savefig(\"plots_netflix_now/scores_%d_%d.jpg\" % (i, j))\n",
    "        else:\n",
    "            plt.plot(range(len(test_out_top[i][j])), test_out_top[i][j], color='red', label='pred. scores')\n",
    "            plt.hlines(top_means_list[i][j][1], -5, len(test_out_top[i][j])+5, linestyles='solid', color='blue')\n",
    "            plt.legend(loc=\"upper right\")\n",
    "            plt.title(\"Time: %d, Item: %d, mean: %f\" % (i, top_items_list_all[i][j], top_means_list[i][j][1]))\n",
    "            plt.xlabel('Users')\n",
    "            plt.ylabel('score')\n",
    "            plt.savefig(\"plots_netflix_now/scores_%d_%d.jpg\" % (i, j))\n",
    "        plt.show()\n",
    "        plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hit_ratio_top_30 = []\n",
    "hit_ratio_top_10 = []\n",
    "hit_ratio_top_5 = []\n",
    "f1_top_30 = []\n",
    "f1_top_10 = []\n",
    "f1_top_5 = []\n",
    "\n",
    "for i, j in zip(top_items_list_75, top_gt_list_next):\n",
    "    hit_ratio_top_30.append(hit_recall(i, j, N=30))\n",
    "    hit_ratio_top_10.append(hit_recall(i, j, N=10))\n",
    "    hit_ratio_top_5.append(hit_recall(i, j, N=5))\n",
    "    \n",
    "    k = np.asarray([i])\n",
    "    q = np.asarray([j])\n",
    "    f1_top_30.append(2*hit_recall(i, j, N=30)**2/(2*hit_recall(i, j, N=30)))\n",
    "    f1_top_10.append(2*hit_recall(i, j, N=10)**2/(2*hit_recall(i, j, N=10)))\n",
    "    try:\n",
    "        f1_top_5.append(2*hit_recall(i, j, N=5)**2/(2*hit_recall(i, j, N=5)))\n",
    "    except ZeroDivisionError:\n",
    "        f1_top_5.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hit_ratio_still_top30 = []\n",
    "hit_ratio_still_top10 = []\n",
    "hit_ratio_still_top5 = []\n",
    "f1_still_30 = []\n",
    "f1_still_10 = []\n",
    "f1_still_5 = []\n",
    "\n",
    "for i, j ,k in zip(top_items_list_75, top_gt_list_now, top_gt_list_next):\n",
    "    still_in_items_30 = []\n",
    "    still_in_items_10 = []\n",
    "    still_in_items_5 = []\n",
    "    \n",
    "    for q in k:\n",
    "        if q in j:\n",
    "            still_in_items_30.append(q)\n",
    "            \n",
    "    for q in k[:10]:\n",
    "        if q in j[:10]:\n",
    "            still_in_items_10.append(q)\n",
    "            \n",
    "    for q in k[:5]:\n",
    "        if q in j[:5]:\n",
    "            still_in_items_5.append(q)\n",
    "            \n",
    "    hit_ratio_still_top30.append(hit_recall(i, still_in_items_30, N=30))\n",
    "    hit_ratio_still_top10.append(hit_recall(i, still_in_items_10, N=10))\n",
    "    hit_ratio_still_top5.append(hit_recall(i, still_in_items_5, N=5))\n",
    "    \n",
    "    x = np.asarray([i])\n",
    "    y_30 = np.asarray([still_in_items_30])\n",
    "    y_10 = np.asarray([still_in_items_10])\n",
    "    y_5 = np.asarray([still_in_items_5])\n",
    "    \n",
    "    f1_still_30.append(2 * hit_recall(i, still_in_items_30, N=30) * \\\n",
    "                       (hit_recall(i, still_in_items_30, N=30)*min(30, len(still_in_items_30))/30) / \\\n",
    "                       (hit_recall(i, still_in_items_30, N=30) + \\\n",
    "                       (hit_recall(i, still_in_items_30, N=30)*min(30, len(still_in_items_30))/30)))\n",
    "    f1_still_10.append(2 * hit_recall(i, still_in_items_10, N=10) * \\\n",
    "                       (hit_recall(i, still_in_items_10, N=10)*min(10, len(still_in_items_10))/10) / \\\n",
    "                       (hit_recall(i, still_in_items_10, N=10) + \\\n",
    "                       (hit_recall(i, still_in_items_10, N=10)*min(10, len(still_in_items_10))/10)))\n",
    "    try:\n",
    "        f1_still_5.append(2 * hit_recall(i, still_in_items_5, N=5) * \\\n",
    "                           (hit_recall(i, still_in_items_5, N=5)*min(5, len(still_in_items_5))/5) / \\\n",
    "                           (hit_recall(i, still_in_items_5, N=5) + \\\n",
    "                           (hit_recall(i, still_in_items_5, N=5)*min(5, len(still_in_items_5))/5)))\n",
    "    except:\n",
    "        f1_still_5.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upper quartile mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(hit_ratio_top_30)), hit_ratio_top_30, color='blue', label='hit_ratios')\n",
    "plt.legend(loc=\"upper right\")\n",
    "# plt.title(\"Time: %d, Item: %d, mean: %f\" % (i, top_items_list[i][j], top_means_list[i][j][1]))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('ratio')\n",
    "plt.title('NOW')\n",
    "plt.show()\n",
    "plt.gcf().clear()\n",
    "print (\"Hit ratio top 30: %f\" % (sum(hit_ratio_top_30[:-1])/len(hit_ratio_top_30[:-1])))\n",
    "print (\"Hit ratio top 10: %f\" % (sum(hit_ratio_top_10[:-1])/len(hit_ratio_top_10[:-1])))\n",
    "print (\"Hit ratio top 5: %f\" % (sum(hit_ratio_top_5[:-1])/len(hit_ratio_top_5[:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(hit_ratio_still_top10)), hit_ratio_still_top10, color='blue', label='hit_ratios')\n",
    "plt.legend(loc=\"upper right\")\n",
    "# plt.title(\"Time: %d, Item: %d, mean: %f\" % (i, top_items_list[i][j], top_means_list[i][j][1]))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('ratio')\n",
    "plt.title('NEXT')\n",
    "plt.show()\n",
    "plt.gcf().clear()\n",
    "print (\"Still in hit ratio top 30: %f\" % (sum(hit_ratio_still_top30[:-1])/len(hit_ratio_still_top30[:-1])))\n",
    "print (\"Still in hit ratio top 10: %f\" % (sum(hit_ratio_still_top10[:-1])/len(hit_ratio_still_top10[:-1])))\n",
    "print (\"Still in hit ratio top 5: %f\" % (sum(hit_ratio_still_top5[:-1])/len(hit_ratio_still_top5[:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"F1 score top 30: %f\" % (sum(f1_top_30[:-1])/len(f1_top_30[:-1])))\n",
    "print (\"F1 score top 10: %f\" % (sum(f1_top_10[:-1])/len(f1_top_10[:-1])))\n",
    "print (\"F1 score top 5: %f\" % (sum(f1_top_5[:-1])/len(f1_top_5[:-1])))\n",
    "print ()\n",
    "print (\"F1 score still top 30: %f\" % (sum(f1_still_30[:-1])/len(f1_still_30[:-1])))\n",
    "print (\"F1 score still top 10: %f\" % (sum(f1_still_10[:-1])/len(f1_still_10[:-1])))\n",
    "print (\"F1 score still top 5: %f\" % (sum(f1_still_5[:-1])/len(f1_still_5[:-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test, _, _ = next(generator)\n",
    "\n",
    "train_data = df_train.as_matrix()\n",
    "test_data = df_test.as_matrix()\n",
    "\n",
    "user_train_rating = np.zeros((total_usr, total_item), dtype=np.float32)\n",
    "for line in train_data:\n",
    "    uid = user_map[line[0]]\n",
    "    iid = item_map[line[1]]\n",
    "    user_train_rating[uid, iid] = 1\n",
    "    \n",
    "item_train_rating = user_train_rating.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.percentile(test_out, 75, axis=0)\n",
    "k = []\n",
    "for i in range(test_out.shape[1]):\n",
    "    k.append(np.mean([x for x in test_out.T[i] if x > a[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (len(sorted(k, reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
