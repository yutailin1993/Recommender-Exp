{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from CDAE import AutoEncoder\n",
    "from tqdm import trange\n",
    "from utils import *\n",
    "from sklearn.cluster import KMeans, spectral_clustering\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_map(list_):\n",
    "    map_ = {}\n",
    "    for idx, ident in enumerate(list_):\n",
    "        map_[ident] = idx\n",
    "        \n",
    "    return map_\n",
    "\n",
    "def get_matrix(data):\n",
    "    matrix = np.zeros((total_usr, total_item), dtype=np.float32)\n",
    "    for line in data:\n",
    "        uid = user_map[line[0]]\n",
    "        iid = item_map[line[1]]\n",
    "        matrix[uid, iid] = 1\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "userList = np.load('../data/netflix/netflix_userList.npy')\n",
    "itemList = np.load('../data/netflix/netflix_itemList.npy')\n",
    "\n",
    "total_usr = len(userList)\n",
    "total_item = len(itemList)\n",
    "\n",
    "user_map = get_map(userList)\n",
    "item_map = get_map(itemList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rating_all = np.load('../data/netflix/train_rating_all.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_indices_all = None\n",
    "test_indices_all = None\n",
    "\n",
    "with open('../data/netflix/train_indices_all.pkl', 'rb') as train_indice_file:\n",
    "    train_indices_all = pickle.load(train_indice_file)\n",
    "    train_indice_file.close()\n",
    "    \n",
    "with open('../data/netflix/test_indices_all.pkl', 'rb') as test_indice_file:\n",
    "    test_indices_all = pickle.load(test_indice_file)\n",
    "    test_indice_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv('../data/class/rating_data.csv')\n",
    "df['freq'] = df.groupby('uid')['uid'].transform('count')  # count frequncy by column's values\n",
    "df = df[df['freq'] > 5]  # remove row which corresponding frequence < 5\n",
    "df_array = df.as_matrix()\n",
    "\n",
    "userList = sorted(df['uid'].unique())\n",
    "itemList = sorted(df['iid'].unique())\n",
    "\n",
    "total_usr = len(df['uid'].unique())\n",
    "total_item = len(df['iid'].unique())\n",
    "\n",
    "user_map = get_map(userList)\n",
    "item_map = get_map(itemList)\n",
    "    \n",
    "\n",
    "sparsity = len(df)/(total_usr*total_item)\n",
    "print(\"sparsity of ratings is %.2f%%\" %(sparsity*100))\n",
    "print (\"num. of users: %d, num. of items: %d, num. of ratings: %d\" % (total_usr, total_item, len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f61543b1860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_vectors = np.load('../data/netflix/feature_vectors.npy')\n",
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "pca_out = pca.fit_transform(user_vectors)\n",
    "colors = np.random.rand(len(pca_out[:, 0]))\n",
    "\n",
    "\n",
    "plt.scatter(pca_out[:, 0], pca_out[:, 1], c=colors, alpha=0.5)\n",
    "plt.savefig('figs/feature_vecPCA_scatter.jpg')\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_vectors = np.load('../data/netflix/feature_vectors.npy')\n",
    "pca = PCA(n_components=10, svd_solver='full')\n",
    "pca_out = pca.fit_transform(user_vectors)\n",
    "NUM_CLUSTER = 30\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTER, n_init=10, algorithm='full')\n",
    "\n",
    "\n",
    "kmeans.fit(pca_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "for i in kmeans.labels_:\n",
    "    if i not in label_map:\n",
    "        label_map[i] = 1\n",
    "    else:\n",
    "        label_map[i] += 1\n",
    "\n",
    "print (label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = {}\n",
    "for i in range(NUM_CLUSTER):\n",
    "    label_index[i] = []\n",
    "    \n",
    "label_list = list(kmeans.labels_)\n",
    "\n",
    "for idx, i in enumerate(label_list):\n",
    "    label_index[i].append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "rating = np.zeros((total_usr, total_item), dtype=np.int8)\n",
    "for line in df_array:\n",
    "    uid = user_map[line[0]]\n",
    "    iid = item_map[line[1]]\n",
    "    rating[uid, iid] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_aps_5 = []\n",
    "test_aps_10 = []\n",
    "test_rec_5 = []\n",
    "test_rec_10 = []\n",
    "\n",
    "for i in range(NUM_CLUSTER):\n",
    "    print (\"Cluster %d.\" % (i))\n",
    "    train_rating = np.take(train_rating_all, label_index[i], axis=0)\n",
    "    train_user = label_index[i]\n",
    "    # train_rating, train_indices, test_indices = gen_train_test(rating_n)\n",
    "    # train_rating = np.take(rating, label_index[i], axis=0)\n",
    "    train_indices = np.take(train_indices_all, label_index[i])\n",
    "    test_indices = np.take(test_indices_all, label_index[i])\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    autoencoder = AutoEncoder(user_num=total_usr, item_num=total_item, mode='user', loss_function='log_loss',\n",
    "                              batch_size=64, epochs=500)\n",
    "    autoencoder.model_load(0)\n",
    "    autoencoder.train(rating=train_rating,\n",
    "                      train_idents=train_user,\n",
    "                      train_indices=train_indices,\n",
    "                      test_indices=test_indices)\n",
    "    \n",
    "    \"\"\"autoencoder.train(rating=rating,\n",
    "                     train_idents=train_user)\"\"\"\n",
    "    \n",
    "    test_ap_5 = autoencoder.log['ap@5']\n",
    "    test_ap_10 = autoencoder.log['ap@10']\n",
    "    recs_5 = autoencoder.log['recall@5']\n",
    "    recs_10 = autoencoder.log['recall@10']\n",
    "    \n",
    "    # top_N_ap = sorted(test_ap,reverse=True)[:20]\n",
    "    # test_aps.append(np.mean(top_N_ap))\n",
    "    \n",
    "    test_aps_5.append(max(test_ap_5))\n",
    "    test_aps_10.append(max(test_ap_10))\n",
    "    test_rec_5.append(max(recs_5))\n",
    "    test_rec_10.append(max(recs_10))\n",
    "    \n",
    "    plt.plot(range(len(test_ap_10)), test_ap_10, color='green', label='Test AP')\n",
    "    # plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"Train 200 epoch\")\n",
    "    plt.xlabel('#Epoch')\n",
    "    plt.ylabel('AP_10')\n",
    "    plt.savefig('./figs/test_ap_cluster_%d.jpg' % (i))\n",
    "    plt.gcf().clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ap_5 = 0\n",
    "\n",
    "for i in range(NUM_CLUSTER):\n",
    "    num = label_map[i]\n",
    "    \n",
    "    ap_5 += test_aps_5[i] * num\n",
    "    print (\"Cluster %d, aps: %f, num: %d, weighted ap: %f\" % (i, test_aps_5[i], num, test_aps_5[i]*num))\n",
    "    \n",
    "ap_5 = ap_5 / (total_usr)\n",
    "\n",
    "print (\"Over all average ap: %f\" % (ap_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_10 = 0\n",
    "\n",
    "for i in range(NUM_CLUSTER):\n",
    "    num = label_map[i]\n",
    "    \n",
    "    ap_10 += test_aps_10[i] * num\n",
    "    print (\"Cluster %d, aps: %f, num: %d, weighted ap: %f\" % (i, test_aps_10[i], num, test_aps_10[i]*num))\n",
    "    \n",
    "ap_10 = ap_10 / (total_usr)\n",
    "\n",
    "print (\"Over all average ap: %f\" % (ap_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_5 = 0\n",
    "\n",
    "for i in range(NUM_CLUSTER):\n",
    "    num = label_map[i]\n",
    "    \n",
    "    recall_5 += test_rec_5[i] * num\n",
    "    print (\"Cluster %d, aps: %f, num: %d, weighted ap: %f\" % (i, test_rec_5[i], num, test_rec_5[i]*num))\n",
    "    \n",
    "recall_5 = recall_5 / (total_usr)\n",
    "\n",
    "print (\"Over all average ap: %f\" % (recall_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_10 = 0\n",
    "\n",
    "for i in range(NUM_CLUSTER):\n",
    "    num = label_map[i]\n",
    "    \n",
    "    recall_10 += test_rec_10[i] * num\n",
    "    print (\"Cluster %d, aps: %f, num: %d, weighted ap: %f\" % (i, test_rec_10[i], num, test_rec_10[i]*num))\n",
    "    \n",
    "recall_10 = recall_10 / (total_usr)\n",
    "\n",
    "print (\"Over all average ap: %f\" % (recall_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For netflix huge dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "user_train_rating = np.load('../data/netflix/rating_matrix_CDAE.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_rating_all, train_indices_all, test_indices_all = gen_train_test(user_train_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "np.save('../data/netflix/train_rating_all.npy', train_rating_all)\n",
    "\n",
    "with open('../data/netflix/train_indices_all.pkl', 'wb') as train_indice_file:\n",
    "    pickle.dump(train_indices_all, train_indice_file)\n",
    "    \n",
    "with open('../data/netflix/test_indices_all.pkl', 'wb') as test_indice_file:\n",
    "    pickle.dump(test_indices_all, test_indice_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data = df.as_matrix()\n",
    "\n",
    "user_train_rating = np.zeros((total_usr, total_item), dtype=np.int32)\n",
    "\n",
    "for line in train_data:\n",
    "    uid = user_map[line[0]]\n",
    "    iid = item_map[line[1]]\n",
    "    user_train_rating[uid, iid] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_rating_all, train_indices_all, test_indices_all = gen_train_test(user_train_rating)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_user_all = np.nonzero(np.count_nonzero(train_rating_all, axis=1))[0]\n",
    "\n",
    "autoencoder = AutoEncoder(user_num=total_usr, item_num=total_item, mode='user', loss_function='log_loss',\n",
    "                          batch_size=100, epochs=1000)\n",
    "\n",
    "\n",
    "autoencoder.train(rating=train_rating_all,\n",
    "                  train_idents=train_user_all,\n",
    "                  train_indices=train_indices_all,\n",
    "                  test_indices=test_indices_all)\n",
    "\n",
    "autoencoder.model_save(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loss = autoencoder.log['train_loss']\n",
    "\n",
    "plt.plot(range(len(train_loss)), train_loss, color='blue', label='Train loss')\n",
    "# plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('#Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ap = autoencoder.log['ap@5']\n",
    "\n",
    "top_N_ap = sorted(test_ap,reverse=True)[:20]\n",
    "\n",
    "plt.plot(range(5, (len(test_ap)+1)*5, 5), test_ap, color='green', label='Test AP')\n",
    "# plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Train 200 epoch\")\n",
    "plt.xlabel('#Epoch')\n",
    "plt.ylabel('AP')\n",
    "plt.show()\n",
    "\n",
    "# print (np.mean(top_N_ap))\n",
    "\n",
    "print (max(test_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_recall = autoencoder.log['recall@10']\n",
    "\n",
    "plt.plot(range(1000, (len(test_recall)+1)*1000, 1000), test_recall, color='green', label='Test Recall')\n",
    "# plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Train 200 epoch\")\n",
    "plt.xlabel('#Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.show()\n",
    "\n",
    "# print (np.mean(top_N_ap))\n",
    "\n",
    "print (max(test_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/cdae_0.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "autoencoder = AutoEncoder(user_num=total_usr, item_num=total_item, mode='user', loss_function='cross_entropy',\n",
    "                          batch_size=100, epochs=1000)\n",
    "autoencoder.model_load(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_onehot_vectors = autoencoder.sess.run(autoencoder.vector_matrix)\n",
    "u_vectors = np.load('../data/netflix/user_vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors = np.zeros((total_usr, 20), dtype=np.float32)\n",
    "for i in range(total_usr):\n",
    "    feature_vectors[i] = autoencoder.sess.run(\n",
    "        autoencoder.code,\n",
    "        feed_dict={\n",
    "            autoencoder.input: [train_rating_all[i]],\n",
    "            autoencoder.ident: [i]\n",
    "        })\n",
    "    \n",
    "np.save('../data/netflix/feature_vectors.npy', feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_onehot_vectors = autoencoder.sess.run(autoencoder.vector_matrix)\n",
    "\n",
    "np.save('../data/netflix/user_vectors.npy', user_onehot_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
