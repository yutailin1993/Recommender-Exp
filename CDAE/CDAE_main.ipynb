{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/larry/Py3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from CDAE import AutoEncoder\n",
    "from tqdm import trange\n",
    "from utils import *\n",
    "from sklearn.cluster import KMeans, spectral_clustering\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def get_map(list_):\n",
    "    map_ = {}\n",
    "    for idx, ident in enumerate(list_):\n",
    "        map_[ident] = idx\n",
    "        \n",
    "    return map_\n",
    "\n",
    "def get_matrix(data):\n",
    "    matrix = np.zeros((total_usr, total_item), dtype=np.float32)\n",
    "    for line in data:\n",
    "        uid = user_map[line[0]]\n",
    "        iid = item_map[line[1]]\n",
    "        matrix[uid, iid] = 1\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def train_test_split(df, time_interval, split_rate=0.5):\n",
    "    start_time = min(df['timestamp'])\n",
    "    end_time = max(df['timestamp'])\n",
    "    time_elapse = (end_time-start_time) // time_interval\n",
    "    split_time = start_time + math.floor(time_elapse * (1-split_rate)) * time_interval\n",
    "    \n",
    "    while split_time < end_time:\n",
    "        df_train = df[df['timestamp'] < split_time]\n",
    "        df_train = df_train[df_train['timestamp'] >= start_time]\n",
    "        \n",
    "        df_test_1 = df[df['timestamp'] >= split_time - 3*time_interval]\n",
    "        df_test_1 = df_test_1[df_test_1['timestamp'] < split_time]\n",
    "        \n",
    "        df_test_2 = df[df['timestamp'] >= split_time]\n",
    "        df_test_2 = df_test_2[df_test_2['timestamp'] < split_time + time_interval]\n",
    "        \n",
    "        # start_time += time_interval\n",
    "        # split_time = start_time + math.floor(time_elapse * (1-split_rate)) * time_interval\n",
    "        split_time += time_interval\n",
    "        \n",
    "        yield df_train, df_test_1, df_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userList = np.load('../data/netflix/netflix_userList.npy')\n",
    "itemList = np.load('../data/netflix/netflix_itemList.npy')\n",
    "\n",
    "total_usr = len(userList)\n",
    "total_item = len(itemList)\n",
    "\n",
    "user_map = get_map(userList)\n",
    "item_map = get_map(itemList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df = pd.read_csv('../data/netflix/rating_netflix_CDAE.csv')\n",
    "df['freq'] = df.groupby('uid')['uid'].transform('count')  # count frequncy by column's values\n",
    "df = df[df['freq'] > 5]  # remove row which corresponding frequence < 5\n",
    "df_array = df.as_matrix()\n",
    "\n",
    "userList = sorted(df['uid'].unique())\n",
    "itemList = sorted(df['iid'].unique())\n",
    "\n",
    "total_usr = len(df['uid'].unique())\n",
    "total_item = len(df['iid'].unique())\n",
    "\n",
    "user_map = get_map(userList)\n",
    "item_map = get_map(itemList)\n",
    "    \n",
    "\n",
    "sparsity = len(df)/(total_usr*total_item)\n",
    "print(\"sparsity of ratings is %.2f%%\" %(sparsity*100))\n",
    "print (\"num. of users: %d, num. of items: %d, num. of ratings: %d\" % (total_usr, total_item, len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_vectors = np.load('../data/class/user_class_vectors.npy')\n",
    "pca = PCA(n_components=10, svd_solver='full')\n",
    "pca_out = pca.fit_transform(user_vectors)\n",
    "NUM_CLUSTER = 10\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTER, n_init=10, algorithm='full')\n",
    "\n",
    "\n",
    "kmeans.fit(pca_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(pca_out.shape[0]):\n",
    "    assert kmeans.predict([pca_out[i]])[0] == kmeans.labels_[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = np.random.rand(len(pca_out[:, 0]))\n",
    "\n",
    "for i in range(pca_out.shape[1]):\n",
    "    plt.scatter(range(pca_out.shape[0]), pca_out[:, i], c=colors, alpha=0.5)\n",
    "    plt.savefig('figs/scatter_%d.jpg' % (i))\n",
    "    plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "for i in kmeans.labels_:\n",
    "    if i not in label_map:\n",
    "        label_map[i] = 1\n",
    "    else:\n",
    "        label_map[i] += 1\n",
    "\n",
    "print (label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_index = {}\n",
    "for i in range(NUM_CLUSTER):\n",
    "    label_index[i] = []\n",
    "    \n",
    "label_list = list(kmeans.labels_)\n",
    "\n",
    "for idx, i in enumerate(label_list):\n",
    "    label_index[i].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rating = np.zeros((total_usr, total_item), dtype=np.float32)\n",
    "for line in df_array:\n",
    "    uid = user_map[line[0]]\n",
    "    iid = item_map[line[1]]\n",
    "    rating[uid, iid] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_aps = []\n",
    "test_rec = []\n",
    "\n",
    "for i in range(NUM_CLUSTER):\n",
    "    # rating_n = np.take(rating, label_index[i], axis=0)\n",
    "    train_user = label_index[i]\n",
    "    # train_rating, train_indices, test_indices = gen_train_test(rating_n)\n",
    "    train_rating = np.take(train_rating_all, label_index[i], axis=0)\n",
    "    train_indices = np.take(train_indices_all, label_index[i])\n",
    "    test_indices = np.take(test_indices_all, label_index[i])\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    autoencoder = AutoEncoder(user_num=total_usr, item_num=total_item, mode='user', loss_function='log_loss',\n",
    "                              batch_size=1, epochs=200)\n",
    "    autoencoder.model_load(0)\n",
    "    autoencoder.train(rating=train_rating,\n",
    "                      train_idents=train_user,\n",
    "                      train_indices=train_indices,\n",
    "                      test_indices=test_indices)\n",
    "    \n",
    "    test_ap = autoencoder.log['ap@5']\n",
    "    recs = autoencoder.log['recall@5']\n",
    "    \n",
    "    # top_N_ap = sorted(test_ap,reverse=True)[:20]\n",
    "    # test_aps.append(np.mean(top_N_ap))\n",
    "    \n",
    "    test_aps.append(max(test_ap))\n",
    "    test_rec.append(max(recs))\n",
    "    \n",
    "    plt.plot(range(len(test_ap)), test_ap, color='green', label='Test AP')\n",
    "    # plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"Train 200 epoch\")\n",
    "    plt.xlabel('#Epoch')\n",
    "    plt.ylabel('AP')\n",
    "    plt.savefig('./figs/test_ap_epochs_%d.jpg' % (i))\n",
    "    plt.gcf().clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ap = 0\n",
    "\n",
    "for i in range(NUM_CLUSTER):\n",
    "    num = label_map[i]\n",
    "    \n",
    "    ap += test_aps[i] * num\n",
    "    print (\"Cluster %d, aps: %f, num: %d, weighted ap: %f\" % (i, test_aps[i], num, test_aps[i]*num))\n",
    "    \n",
    "ap = ap / (total_usr)\n",
    "\n",
    "print (\"Over all average ap: %f\" % (ap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_train_rating = np.load('../data/netflix/rating_matrix_CDAE.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "train_data = df_array\n",
    "\n",
    "user_train_rating = np.zeros((total_usr, total_item), dtype=np.int8)\n",
    "\n",
    "for line in train_data:\n",
    "    uid = user_map[line[0]]\n",
    "    iid = item_map[line[1]]\n",
    "    user_train_rating[uid, iid] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-32fb7169af07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m autoencoder.train(rating=user_train_rating,\n\u001b[0;32m---> 12\u001b[0;31m                   train_idents=train_user_all)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/experiment/Recommender Exp/CDAE/CDAE.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, rating, train_idents, train_indices, test_indices)\u001b[0m\n\u001b[1;32m    191\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mident\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0midents_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                         })\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_rating_all, train_indices_all, test_indices_all = gen_train_test(user_train_rating)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_user_all = np.nonzero(np.count_nonzero(user_train_rating, axis=1))[0]\n",
    "\n",
    "autoencoder = AutoEncoder(user_num=total_usr, item_num=total_item, mode='user', loss_function='log_loss',\n",
    "                          batch_size=100, epochs=200)\n",
    "\n",
    "\n",
    "autoencoder.train(rating=user_train_rating,\n",
    "                  train_idents=train_user_all)\n",
    "\n",
    "autoencoder.model_save(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loss = autoencoder.log['train_loss']\n",
    "\n",
    "plt.plot(range(len(train_loss)), train_loss, color='blue', label='Train loss')\n",
    "# plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('#Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ap = autoencoder.log['ap@5']\n",
    "\n",
    "top_N_ap = sorted(test_ap,reverse=True)[:20]\n",
    "\n",
    "plt.plot(range(len(test_ap)), test_ap, color='green', label='Test AP')\n",
    "# plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Train 200 epoch\")\n",
    "plt.xlabel('#Epoch')\n",
    "plt.ylabel('AP')\n",
    "plt.show()\n",
    "\n",
    "# print (np.mean(top_N_ap))\n",
    "\n",
    "print (max(test_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_onehot_vectors = autoencoder.sess.run(autoencoder.vector_matrix)\n",
    "\n",
    "np.save('../data/class/user_class_vectors.npy', user_onehot_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_rating, train_indices, test_indices = gen_train_test(item_rating)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "autoencoder = AutoEncoder(user_num=total_usr, item_num=total_item, mode='item', loss_function='log_loss',\n",
    "                          epochs=200)\n",
    "autoencoder.train(rating=train_rating,\n",
    "                  train_indices=train_indices,\n",
    "                  test_indices=test_indices)\n",
    "\n",
    "train_loss = autoencoder.log['train_loss']\n",
    "\n",
    "plt.plot(range(len(train_loss)), train_loss, color='blue', label='Train loss')\n",
    "# plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('#Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ap = autoencoder.log['ap@5']\n",
    "\n",
    "plt.plot(range(len(test_ap)), test_ap, color='green', label='Test AP')\n",
    "# plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Train 5000 epoch\")\n",
    "plt.xlabel('#Epoch')\n",
    "plt.ylabel('AP')\n",
    "plt.show()\n",
    "\n",
    "print (test_ap.index(max(test_ap)), max(test_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_onehot_vectors = autoencoder.sess.run(autoencoder.vector_matrix)\n",
    "\n",
    "np.save('../data/itri/item_itri_non_pruned_vectors.npy', item_onehot_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP@5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recon = autoencoder.decode.eval(\n",
    "    session=autoencoder.sess,\n",
    "    feed_dict={\n",
    "        autoencoder.input: [train_rating[1]],\n",
    "        autoencoder.ident: 1\n",
    "    })\n",
    "\n",
    "print (recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aps = []\n",
    "\n",
    "for usr in range(user_rating.shape[0]):\n",
    "    recon = autoencoder.decode.eval(\n",
    "        session=autoencoder.sess,\n",
    "        feed_dict={\n",
    "            autoencoder.input: [train_rating[usr]],\n",
    "            autoencoder.ident: usr\n",
    "        })\n",
    "    \n",
    "    top5 = get_topN(recon, train_indices[usr])\n",
    "    \n",
    "    aps.append(avg_precision(top5, test_indices[usr]))\n",
    "\n",
    "print (\"MAP: %f\" % (sum(aps)/len(aps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP@5 Top10 and HitRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pop_top10 = rank[:10]\n",
    "pop_top20 = rank[:20]\n",
    "pop10_aps = []\n",
    "hit_rates_10 = []\n",
    "pop20_aps = []\n",
    "hit_rates_20 = []\n",
    "\n",
    "def hit_ratio(topN, indices):\n",
    "    N = len(topN)\n",
    "    hit_count = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        hit_count += 1 if topN[i] in indices else 0\n",
    "    \n",
    "    try:\n",
    "        return hit_count / min(N, len(indices))\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "for usr in range(user_rating.shape[0]):\n",
    "    recon = autoencoder.decode.eval(\n",
    "        session=autoencoder.sess,\n",
    "        feed_dict={\n",
    "            autoencoder.input: [train_rating[usr]],\n",
    "            autoencoder.ident: usr\n",
    "        })\n",
    "    \n",
    "    top5 = get_topN(recon, train_indices[usr])\n",
    "    \n",
    "    pop10_top5 = [x for x in top5 if x in pop_top10]\n",
    "    pop10_test_index = [x for x in test_indices[usr] if x in pop_top10]\n",
    "    \n",
    "    pop20_top5 = [x for x in top5 if x in pop_top20]\n",
    "    pop20_test_index = [x for x in test_indices[usr] if x in pop_top20]\n",
    "    \n",
    "    rate_10 = hit_ratio(pop10_top5, pop10_test_index)\n",
    "    ap_10 = avg_precision(pop10_top5, pop10_test_index)\n",
    "    \n",
    "    rate_20 = hit_ratio(pop20_top5, pop20_test_index)\n",
    "    ap_20 = avg_precision(pop20_top5, pop20_test_index)\n",
    "    \n",
    "    if ap_10 <= 1:\n",
    "        pop10_aps.append(ap_10)\n",
    "        hit_rates_10.append(rate_10)\n",
    "        \n",
    "    if ap_20 <= 1:\n",
    "        pop20_aps.append(ap_20)\n",
    "        hit_rates_20.append(rate_20)\n",
    "\n",
    "\n",
    "print (\"MAP in Popular Top 10: {0:.3f}%\".format(sum(pop10_aps)/len(pop10_aps)*100))\n",
    "print (\"HIT_RATE in Popular Top 10: {0:.3f}%\".format(sum(hit_rates_10)/len(hit_rates_10)*100))\n",
    "print ()\n",
    "print (\"MAP in Popular Top 20: {0:.3f}%\".format(sum(pop20_aps)/len(pop20_aps)*100))\n",
    "print (\"HIT_RATE in Popular Top 20: {0:.3f}%\".format(sum(hit_rates_20)/len(hit_rates_20)*100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
