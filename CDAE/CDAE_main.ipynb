{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/larry/Py3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from CDAE import AutoEncoder\n",
    "from tqdm import trange\n",
    "from utils import *\n",
    "from sklearn.cluster import KMeans, spectral_clustering\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def get_map(list_):\n",
    "    map_ = {}\n",
    "    for idx, ident in enumerate(list_):\n",
    "        map_[ident] = idx\n",
    "        \n",
    "    return map_\n",
    "\n",
    "def get_matrix(data):\n",
    "    matrix = np.zeros((total_usr, total_item), dtype=np.float32)\n",
    "    for line in data:\n",
    "        uid = user_map[line[0]]\n",
    "        iid = item_map[line[1]]\n",
    "        matrix[uid, iid] = 1\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "userList = np.load('../data/netflix/netflix_userList.npy')\n",
    "itemList = np.load('../data/netflix/netflix_itemList.npy')\n",
    "\n",
    "total_usr = len(userList)\n",
    "total_item = len(itemList)\n",
    "\n",
    "user_map = get_map(userList)\n",
    "item_map = get_map(itemList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_rating_all = np.load('../data/netflix/train_rating_all.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "train_indices_all = None\n",
    "test_indices_all = None\n",
    "\n",
    "with open('../data/netflix/train_indices_all.pkl', 'rb') as train_indice_file:\n",
    "    train_indices_all = pickle.load(train_indice_file)\n",
    "    train_indice_file.close()\n",
    "    \n",
    "with open('../data/netflix/test_indices_all.pkl', 'rb') as test_indice_file:\n",
    "    test_indices_all = pickle.load(test_indice_file)\n",
    "    test_indice_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity of ratings is 5.16%\n",
      "num. of users: 276, num. of items: 989, num. of ratings: 14072\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/itri/rating_itri_pruned.csv')\n",
    "df['freq'] = df.groupby('uid')['uid'].transform('count')  # count frequncy by column's values\n",
    "df = df[df['freq'] > 5]  # remove row which corresponding frequence < 5\n",
    "df_array = df.as_matrix()\n",
    "\n",
    "userList = sorted(df['uid'].unique())\n",
    "itemList = sorted(df['iid'].unique())\n",
    "\n",
    "total_usr = len(df['uid'].unique())\n",
    "total_item = len(df['iid'].unique())\n",
    "\n",
    "user_map = get_map(userList)\n",
    "item_map = get_map(itemList)\n",
    "    \n",
    "\n",
    "sparsity = len(df)/(total_usr*total_item)\n",
    "print(\"sparsity of ratings is %.2f%%\" %(sparsity*100))\n",
    "print (\"num. of users: %d, num. of items: %d, num. of ratings: %d\" % (total_usr, total_item, len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_vectors = np.load('../data/itri/feature_vectors.npy')\n",
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "pca_out = pca.fit_transform(user_vectors)\n",
    "colors = np.random.rand(len(pca_out[:, 0]))\n",
    "\n",
    "\n",
    "plt.scatter(pca_out[:, 0], pca_out[:, 1], c=colors, alpha=0.5)\n",
    "plt.savefig('figs/feature_vecPCA_scatter.jpg')\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_vectors = np.load('../data/netflix/feature_vectors.npy')\n",
    "pca = PCA(n_components=10, svd_solver='full')\n",
    "pca_out = pca.fit_transform(user_vectors)\n",
    "NUM_CLUSTER = 50\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTER, n_init=10, algorithm='full')\n",
    "\n",
    "\n",
    "kmeans.fit(pca_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "for i in kmeans.labels_:\n",
    "    if i not in label_map:\n",
    "        label_map[i] = 1\n",
    "    else:\n",
    "        label_map[i] += 1\n",
    "\n",
    "print (label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_index = {}\n",
    "for i in range(NUM_CLUSTER):\n",
    "    label_index[i] = []\n",
    "    \n",
    "label_list = list(kmeans.labels_)\n",
    "\n",
    "for idx, i in enumerate(label_list):\n",
    "    label_index[i].append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "rating = np.zeros((total_usr, total_item), dtype=np.int8)\n",
    "for line in df_array:\n",
    "    uid = user_map[line[0]]\n",
    "    iid = item_map[line[1]]\n",
    "    rating[uid, iid] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_aps_5 = []\n",
    "test_aps_10 = []\n",
    "test_rec_5 = []\n",
    "test_rec_10 = []\n",
    "\n",
    "for i in range(NUM_CLUSTER):\n",
    "    print (\"Cluster %d.\" % (i))\n",
    "    train_rating = np.take(train_rating_all, label_index[i], axis=0)\n",
    "    train_user = label_index[i]\n",
    "    # train_rating, train_indices, test_indices = gen_train_test(rating_n)\n",
    "    # train_rating = np.take(rating, label_index[i], axis=0)\n",
    "    train_indices = np.take(train_indices_all, label_index[i])\n",
    "    test_indices = np.take(test_indices_all, label_index[i])\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    autoencoder = AutoEncoder(user_num=total_usr, item_num=total_item, mode='user', loss_function='log_loss',\n",
    "                              batch_size=64, epochs=500)\n",
    "    autoencoder.model_load(0)\n",
    "    autoencoder.train(rating=train_rating,\n",
    "                      train_idents=train_user,\n",
    "                      train_indices=train_indices,\n",
    "                      test_indices=test_indices)\n",
    "    \n",
    "    \"\"\"autoencoder.train(rating=rating,\n",
    "                     train_idents=train_user)\"\"\"\n",
    "    \n",
    "    test_ap_5 = autoencoder.log['ap@5']\n",
    "    test_ap_10 = autoencoder.log['ap@10']\n",
    "    recs_5 = autoencoder.log['recall@5']\n",
    "    recs_10 = autoencoder.log['recall@10']\n",
    "    \n",
    "    # top_N_ap = sorted(test_ap,reverse=True)[:20]\n",
    "    # test_aps.append(np.mean(top_N_ap))\n",
    "    \n",
    "    test_aps_5.append(max(test_ap_5))\n",
    "    test_aps_10.append(max(test_ap_10))\n",
    "    test_rec_5.append(max(recs_5))\n",
    "    test_rec_10.append(max(recs_10))\n",
    "    \n",
    "    plt.plot(range(len(test_ap_10)), test_ap_10, color='green', label='Test AP')\n",
    "    # plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"Train 200 epoch\")\n",
    "    plt.xlabel('#Epoch')\n",
    "    plt.ylabel('AP_10')\n",
    "    plt.savefig('./figs/test_ap_cluster_%d.jpg' % (i))\n",
    "    plt.gcf().clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ap_5 = 0\n",
    "\n",
    "for i in range(NUM_CLUSTER):\n",
    "    num = label_map[i]\n",
    "    \n",
    "    ap_5 += test_aps_5[i] * num\n",
    "    print (\"Cluster %d, aps: %f, num: %d, weighted ap: %f\" % (i, test_aps_5[i], num, test_aps_5[i]*num))\n",
    "    \n",
    "ap_5 = ap_5 / (total_usr)\n",
    "\n",
    "print (\"Over all average ap: %f\" % (ap_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ap_10 = 0\n",
    "\n",
    "for i in range(NUM_CLUSTER):\n",
    "    num = label_map[i]\n",
    "    \n",
    "    ap_10 += test_aps_10[i] * num\n",
    "    print (\"Cluster %d, aps: %f, num: %d, weighted ap: %f\" % (i, test_aps_10[i], num, test_aps_10[i]*num))\n",
    "    \n",
    "ap_10 = ap_10 / (total_usr)\n",
    "\n",
    "print (\"Over all average ap: %f\" % (ap_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recall_5 = 0\n",
    "\n",
    "for i in range(NUM_CLUSTER):\n",
    "    num = label_map[i]\n",
    "    \n",
    "    recall_5 += test_rec_5[i] * num\n",
    "    print (\"Cluster %d, aps: %f, num: %d, weighted ap: %f\" % (i, test_rec_5[i], num, test_rec_5[i]*num))\n",
    "    \n",
    "recall_5 = recall_5 / (total_usr)\n",
    "\n",
    "print (\"Over all average ap: %f\" % (recall_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recall_10 = 0\n",
    "\n",
    "for i in range(NUM_CLUSTER):\n",
    "    num = label_map[i]\n",
    "    \n",
    "    recall_10 += test_rec_10[i] * num\n",
    "    print (\"Cluster %d, aps: %f, num: %d, weighted ap: %f\" % (i, test_rec_10[i], num, test_rec_10[i]*num))\n",
    "    \n",
    "recall_10 = recall_10 / (total_usr)\n",
    "\n",
    "print (\"Over all average ap: %f\" % (recall_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For netflix huge dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "user_train_rating = np.load('../data/netflix/rating_matrix_CDAE.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_rating_all, train_indices_all, test_indices_all = gen_train_test(user_train_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "np.save('../data/netflix/train_rating_all.npy', train_rating_all)\n",
    "\n",
    "with open('../data/netflix/train_indices_all.pkl', 'wb') as train_indice_file:\n",
    "    pickle.dump(train_indices_all, train_indice_file)\n",
    "    \n",
    "with open('../data/netflix/test_indices_all.pkl', 'wb') as test_indice_file:\n",
    "    pickle.dump(test_indices_all, test_indice_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = df.as_matrix()\n",
    "\n",
    "user_train_rating = np.zeros((total_usr, total_item), dtype=np.int32)\n",
    "\n",
    "for line in train_data:\n",
    "    uid = user_map[line[0]]\n",
    "    iid = item_map[line[1]]\n",
    "    user_train_rating[uid, iid] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topN = np.count_nonzero(user_train_rating, axis=0).argsort()[::-1][:10]\n",
    "others = [k for k in range(total_item) if k not in topN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_train(inputs, train_user_all, train_indices, test_indices, topN=None, weight=None):\n",
    "    logs = {'ap@5': [], 'ap@10': [], 'ap@5_top': [], 'ap@10_top': [], 'filter_loss': []}\n",
    "    \n",
    "    if topN is not None:\n",
    "        filter_ = np.ones(total_item, dtype=np.int32)\n",
    "        for i in range(filter_.shape[0]):\n",
    "            if i in topN:\n",
    "                filter_[i] = weight\n",
    "    else:\n",
    "        filter_ = np.ones(total_item, dtype=np.int8)\n",
    "        \n",
    "    train_idents_idx = [k for k in range(len(train_user_all))]\n",
    "        \n",
    "    for epoch in trange(500):\n",
    "        # total_loss = 0\n",
    "        ap_at_5 = []\n",
    "        ap_at_5_top = []\n",
    "        ap_at_10 = []\n",
    "        ap_at_10_top = []\n",
    "        \n",
    "        for n in train_user_all:\n",
    "            input_ = np.take(inputs, train_idents_idx[n:n+1], axis=0)\n",
    "            target_ = input_\n",
    "            idents_ = train_user_all[n:n+1]\n",
    "            \n",
    "            recon, filter_loss, _ = autoencoder.sess.run(\n",
    "                [autoencoder.decode, autoencoder.filter_loss, autoencoder.optim],\n",
    "                feed_dict={\n",
    "                    autoencoder.input: input_,\n",
    "                    autoencoder.target: target_,\n",
    "                    autoencoder.ident: idents_,\n",
    "                    autoencoder.filter: filter_,\n",
    "                })\n",
    "            \n",
    "            top10 = get_topN(recon, train_indices[n:n+1], N=10)\n",
    "            top5 = get_topN(recon, train_indices[n:n+1], N=5)\n",
    "            \n",
    "            top10_top = np.asarray([[k for k in test_indices[n] if k in topN]])\n",
    "            top5_top = np.asarray([[k for k in  test_indices[n] if k in topN]])\n",
    "            \n",
    "            iAP_5 = avg_precision(top5, test_indices[n:n+1])\n",
    "            iAP_10 = avg_precision(top10, test_indices[n:n+1])\n",
    "            iAP_5_top = avg_precision(top5, top5_top)\n",
    "            iAP_10_top = avg_precision(top10, top10_top)\n",
    "            \n",
    "            if iAP_5 is not None:\n",
    "                ap_at_5.append(iAP_5)  \n",
    "            if iAP_10 is not None:\n",
    "                ap_at_10.append(iAP_10)\n",
    "            if iAP_5_top is not None:\n",
    "                ap_at_5_top.append(iAP_5_top)\n",
    "            if iAP_10_top is not None:\n",
    "                ap_at_10_top.append(iAP_10_top)\n",
    "                \n",
    "        logs['ap@5'].append(sum(ap_at_5)/len(ap_at_5))\n",
    "        logs['ap@10'].append(sum(ap_at_10)/len(ap_at_10))\n",
    "        logs['ap@5_top'].append(sum(ap_at_5_top)/len(ap_at_5_top))\n",
    "        logs['ap@10_top'].append(sum(ap_at_10_top)/len(ap_at_10_top))\n",
    "        logs['filter_loss'].append(filter_loss)\n",
    "        \n",
    "    logs['filter_loss'] = np.asarray(logs['filter_loss'])\n",
    "        \n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:34<00:00,  2.33it/s]\n",
      "100%|██████████| 500/500 [03:34<00:00,  2.34it/s]\n",
      "100%|██████████| 500/500 [03:34<00:00,  2.33it/s]\n",
      "100%|██████████| 500/500 [03:35<00:00,  2.32it/s]\n"
     ]
    }
   ],
   "source": [
    "weights = [1, 0.1, 0.01, 0.001]\n",
    "\n",
    "for s in weights:\n",
    "    train_rating_all, train_indices_all, test_indices_all = gen_train_test(user_train_rating)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    train_user_all = np.nonzero(np.count_nonzero(train_rating_all, axis=1))[0]\n",
    "\n",
    "    autoencoder = AutoEncoder(user_num=total_usr, item_num=total_item, mode='user', with_weights=True, loss_function='cross_entropy',\n",
    "                              batch_size=1, epochs=500)\n",
    "    \n",
    "    logs = train_train(train_rating_all, train_user_all, train_indices_all, test_indices_all, topN=topN, weight=s)\n",
    "    \n",
    "    test_ap = logs['ap@5']\n",
    "\n",
    "    plt.plot(range((len(test_ap))), test_ap, color='green', label='Test AP')\n",
    "    # plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"test_ap@5_with_weight_%3f.jpg'\" % (s))\n",
    "    plt.xlabel('#Epoch')\n",
    "    plt.ylabel('AP')\n",
    "    plt.savefig('./figs/test_ap@5_with_weight_%3f.jpg' % (s))\n",
    "    plt.gcf().clear()\n",
    "    \n",
    "    test_ap = logs['ap@10']\n",
    "\n",
    "    plt.plot(range((len(test_ap))), test_ap, color='green', label='Test AP')\n",
    "    # plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"test_ap@10_with_weight_%3f.jpg'\" % (s))\n",
    "    plt.xlabel('#Epoch')\n",
    "    plt.ylabel('AP')\n",
    "    plt.savefig('./figs/test_ap@10_with_weight_%3f.jpg' % (s))\n",
    "    plt.gcf().clear()\n",
    "    \n",
    "    test_ap = logs['ap@5_top']\n",
    "\n",
    "    plt.plot(range((len(test_ap))), test_ap, color='green', label='Test AP')\n",
    "    # plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"test_ap@5_top_with_weight_%3f.jpg'\" % (s))\n",
    "    plt.xlabel('#Epoch')\n",
    "    plt.ylabel('AP')\n",
    "    plt.savefig('./figs/test_ap@5_top_with_weight_%3f.jpg' % (s))\n",
    "    plt.gcf().clear()\n",
    "    \n",
    "    test_ap = logs['ap@10_top']\n",
    "\n",
    "    plt.plot(range((len(test_ap))), test_ap, color='green', label='Test AP')\n",
    "    # plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"test_ap@10_top_with_weight_%3f.jpg'\" % (s))\n",
    "    plt.xlabel('#Epoch')\n",
    "    plt.ylabel('AP')\n",
    "    plt.savefig('./figs/test_ap@10_top_with_weight_%3f.jpg' % (s))\n",
    "    plt.gcf().clear()\n",
    "    \n",
    "    topN_loss = []\n",
    "    other_loss = []\n",
    "\n",
    "    for i in logs['filter_loss']:\n",
    "        topN_loss.append(sum(sum(np.take(i, topN, axis=1))))\n",
    "        other_loss.append(sum(sum(np.take(i, others, axis=1))))\n",
    "        \n",
    "    plt.plot(range((len(topN_loss))), topN_loss, color='blue', label='topN Loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"topN_loss_with_weight_%3f\" % (s))\n",
    "    plt.xlabel('#Epoch')\n",
    "    plt.ylabel('cross entropy')\n",
    "    plt.savefig('./figs/topN_loss_with_weight_%3f.jpg' % (s))\n",
    "    plt.gcf().clear()\n",
    "    \n",
    "    plt.plot(range((len(other_loss))), other_loss, color='blue', label='other Loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"other_loss_with_weight_%3f\" % (s))\n",
    "    plt.xlabel('#Epoch')\n",
    "    plt.ylabel('cross entropy')\n",
    "    plt.savefig('./figs/other_loss_with_weight_%3f.jpg' % (s))\n",
    "    plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rating_all, train_indices_all, test_indices_all = gen_train_test(user_train_rating)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_user_all = np.nonzero(np.count_nonzero(train_rating_all, axis=1))[0]\n",
    "\n",
    "autoencoder = AutoEncoder(user_num=total_usr, item_num=total_item, mode='user', with_weights=True, loss_function='cross_entropy',\n",
    "                          batch_size=1, epochs=500)\n",
    "\n",
    "\n",
    "autoencoder.train(rating=train_rating_all,\n",
    "                  train_idents=train_user_all,\n",
    "                  train_indices=train_indices_all,\n",
    "                  test_indices=test_indices_all,\n",
    "                  topN=topN,\n",
    "                  weight=10000)\n",
    "\n",
    "# autoencoder.model_save(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loss = autoencoder.log['train_loss']\n",
    "\n",
    "plt.plot(range(len(train_loss)), train_loss, color='blue', label='Train loss')\n",
    "# plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('#Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ap = autoencoder.log['ap@5']\n",
    "\n",
    "plt.plot(range((len(test_ap))), test_ap, color='green', label='Test AP')\n",
    "# plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Train 200 epoch\")\n",
    "plt.xlabel('#Epoch')\n",
    "plt.ylabel('AP')\n",
    "plt.show()\n",
    "\n",
    "# print (np.mean(top_N_ap))\n",
    "\n",
    "print (max(test_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_recall = autoencoder.log['recall@10']\n",
    "\n",
    "plt.plot(range(1000, (len(test_recall)+1)*1000, 1000), test_recall, color='green', label='Test Recall')\n",
    "# plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Train 200 epoch\")\n",
    "plt.xlabel('#Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.show()\n",
    "\n",
    "# print (np.mean(top_N_ap))\n",
    "\n",
    "print (max(test_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "autoencoder = AutoEncoder(user_num=total_usr, item_num=total_item, mode='user', loss_function='cross_entropy',\n",
    "                          batch_size=100, epochs=1000)\n",
    "autoencoder.model_load(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_onehot_vectors = autoencoder.sess.run(autoencoder.vector_matrix)\n",
    "u_vectors = np.load('../data/netflix/user_vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_vectors = np.zeros((total_usr, 20), dtype=np.float32)\n",
    "for i in range(total_usr):\n",
    "    feature_vectors[i] = autoencoder.sess.run(\n",
    "        autoencoder.code,\n",
    "        feed_dict={\n",
    "            autoencoder.input: [train_rating_all[i]],\n",
    "            autoencoder.ident: [i]\n",
    "        })\n",
    "    \n",
    "np.save('../data/netflix/feature_vectors.npy', feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_onehot_vectors = autoencoder.sess.run(autoencoder.vector_matrix)\n",
    "\n",
    "np.save('../data/netflix/user_vectors.npy', user_onehot_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
