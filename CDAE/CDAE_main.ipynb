{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from CDAE import AutoEncoder\n",
    "from tqdm import trange\n",
    "from utils import *\n",
    "from sklearn.cluster import KMeans, spectral_clustering\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_map(list_):\n",
    "    map_ = {}\n",
    "    for idx, ident in enumerate(list_):\n",
    "        map_[ident] = idx\n",
    "        \n",
    "    return map_\n",
    "\n",
    "def get_matrix(data):\n",
    "    matrix = np.zeros((total_usr, total_item), dtype=np.float32)\n",
    "    for line in data:\n",
    "        uid = user_map[line[0]]\n",
    "        iid = item_map[line[1]]\n",
    "        matrix[uid, iid] = 1\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "userList = np.load('../data/netflix/netflix_userList.npy')\n",
    "itemList = np.load('../data/netflix/netflix_itemList.npy')\n",
    "\n",
    "total_usr = len(userList)\n",
    "total_item = len(itemList)\n",
    "\n",
    "user_map = get_map(userList)\n",
    "item_map = get_map(itemList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rating_all = np.load('../data/netflix/train_rating_all.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_indices_all = None\n",
    "test_indices_all = None\n",
    "\n",
    "with open('../data/netflix/train_indices_all.pkl', 'rb') as train_indice_file:\n",
    "    train_indices_all = pickle.load(train_indice_file)\n",
    "    train_indice_file.close()\n",
    "    \n",
    "with open('../data/netflix/test_indices_all.pkl', 'rb') as test_indice_file:\n",
    "    test_indices_all = pickle.load(test_indice_file)\n",
    "    test_indice_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv('../data/class/rating_data.csv')\n",
    "df['freq'] = df.groupby('uid')['uid'].transform('count')  # count frequncy by column's values\n",
    "df = df[df['freq'] > 5]  # remove row which corresponding frequence < 5\n",
    "df_array = df.as_matrix()\n",
    "\n",
    "userList = sorted(df['uid'].unique())\n",
    "itemList = sorted(df['iid'].unique())\n",
    "\n",
    "total_usr = len(df['uid'].unique())\n",
    "total_item = len(df['iid'].unique())\n",
    "\n",
    "user_map = get_map(userList)\n",
    "item_map = get_map(itemList)\n",
    "    \n",
    "\n",
    "sparsity = len(df)/(total_usr*total_item)\n",
    "print(\"sparsity of ratings is %.2f%%\" %(sparsity*100))\n",
    "print (\"num. of users: %d, num. of items: %d, num. of ratings: %d\" % (total_usr, total_item, len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vectors = np.load('../data/netflix/feature_vectors.npy')\n",
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "pca_out = pca.fit_transform(user_vectors)\n",
    "colors = np.random.rand(len(pca_out[:, 0]))\n",
    "\n",
    "\n",
    "plt.scatter(pca_out[:, 0], pca_out[:, 1], c=colors, alpha=0.5)\n",
    "plt.savefig('figs/feature_vecPCA_scatter.jpg')\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='full', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=50, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_vectors = np.load('../data/netflix/feature_vectors.npy')\n",
    "pca = PCA(n_components=10, svd_solver='full')\n",
    "pca_out = pca.fit_transform(user_vectors)\n",
    "NUM_CLUSTER = 50\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTER, n_init=10, algorithm='full')\n",
    "\n",
    "\n",
    "kmeans.fit(pca_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 221157, 7: 2615, 13: 15717, 10: 4333, 47: 7666, 29: 947, 15: 181, 3: 1704, 6: 300, 11: 135, 25: 4, 1: 103, 19: 540, 38: 88, 28: 1, 12: 161, 21: 83, 26: 56, 4: 40, 49: 341, 24: 6, 37: 1, 48: 33, 42: 31, 40: 179, 43: 8, 41: 15, 33: 31, 18: 1, 2: 15, 32: 20, 45: 17, 20: 44, 16: 29, 39: 10, 22: 16, 8: 5, 23: 1, 14: 10, 35: 2, 44: 16, 36: 2, 5: 1, 27: 2, 17: 9, 31: 1, 30: 3, 9: 1, 34: 1, 46: 1}\n"
     ]
    }
   ],
   "source": [
    "label_map = {}\n",
    "for i in kmeans.labels_:\n",
    "    if i not in label_map:\n",
    "        label_map[i] = 1\n",
    "    else:\n",
    "        label_map[i] += 1\n",
    "\n",
    "print (label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = {}\n",
    "for i in range(NUM_CLUSTER):\n",
    "    label_index[i] = []\n",
    "    \n",
    "label_list = list(kmeans.labels_)\n",
    "\n",
    "for idx, i in enumerate(label_list):\n",
    "    label_index[i].append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "rating = np.zeros((total_usr, total_item), dtype=np.int8)\n",
    "for line in df_array:\n",
    "    uid = user_map[line[0]]\n",
    "    iid = item_map[line[1]]\n",
    "    rating[uid, iid] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0.\n",
      "INFO:tensorflow:Restoring parameters from model/cdae_0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/500 [01:53<3:18:01, 24.00s/it]"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1700406bf3cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                       \u001b[0mtrain_idents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_user\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                       \u001b[0mtrain_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                       test_indices=test_indices)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \"\"\"autoencoder.train(rating=rating,\n",
      "\u001b[0;32m~/Desktop/experiment/Recommender-Exp/CDAE/CDAE.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, rating, train_idents, train_indices, test_indices)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ap@5'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map_at_5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map_at_5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ap@10'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map_at_10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map_at_10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recall@5'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_at_5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_at_5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "test_aps_5 = []\n",
    "test_aps_10 = []\n",
    "test_rec_5 = []\n",
    "test_rec_10 = []\n",
    "\n",
    "for i in range(NUM_CLUSTER):\n",
    "    print (\"Cluster %d.\" % (i))\n",
    "    train_rating = np.take(train_rating_all, label_index[i], axis=0)\n",
    "    train_user = label_index[i]\n",
    "    # train_rating, train_indices, test_indices = gen_train_test(rating_n)\n",
    "    # train_rating = np.take(rating, label_index[i], axis=0)\n",
    "    train_indices = np.take(train_indices_all, label_index[i])\n",
    "    test_indices = np.take(test_indices_all, label_index[i])\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    autoencoder = AutoEncoder(user_num=total_usr, item_num=total_item, mode='user', loss_function='log_loss',\n",
    "                              batch_size=64, epochs=500)\n",
    "    autoencoder.model_load(0)\n",
    "    autoencoder.train(rating=train_rating,\n",
    "                      train_idents=train_user,\n",
    "                      train_indices=train_indices,\n",
    "                      test_indices=test_indices)\n",
    "    \n",
    "    \"\"\"autoencoder.train(rating=rating,\n",
    "                     train_idents=train_user)\"\"\"\n",
    "    \n",
    "    test_ap_5 = autoencoder.log['ap@5']\n",
    "    test_ap_10 = autoencoder.log['ap@10']\n",
    "    recs_5 = autoencoder.log['recall@5']\n",
    "    recs_10 = autoencoder.log['recall@10']\n",
    "    \n",
    "    # top_N_ap = sorted(test_ap,reverse=True)[:20]\n",
    "    # test_aps.append(np.mean(top_N_ap))\n",
    "    \n",
    "    test_aps_5.append(max(test_ap_5))\n",
    "    test_aps_10.append(max(test_ap_10))\n",
    "    test_rec_5.append(max(recs_5))\n",
    "    test_rec_10.append(max(recs_10))\n",
    "    \n",
    "    plt.plot(range(len(test_ap_10)), test_ap_10, color='green', label='Test AP')\n",
    "    # plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"Train 200 epoch\")\n",
    "    plt.xlabel('#Epoch')\n",
    "    plt.ylabel('AP_10')\n",
    "    plt.savefig('./figs/test_ap_cluster_%d.jpg' % (i))\n",
    "    plt.gcf().clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ap_5 = 0\n",
    "\n",
    "for i in range(NUM_CLUSTER):\n",
    "    num = label_map[i]\n",
    "    \n",
    "    ap_5 += test_aps_5[i] * num\n",
    "    print (\"Cluster %d, aps: %f, num: %d, weighted ap: %f\" % (i, test_aps_5[i], num, test_aps_5[i]*num))\n",
    "    \n",
    "ap_5 = ap_5 / (total_usr)\n",
    "\n",
    "print (\"Over all average ap: %f\" % (ap_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_10 = 0\n",
    "\n",
    "for i in range(NUM_CLUSTER):\n",
    "    num = label_map[i]\n",
    "    \n",
    "    ap_10 += test_aps_10[i] * num\n",
    "    print (\"Cluster %d, aps: %f, num: %d, weighted ap: %f\" % (i, test_aps_10[i], num, test_aps_10[i]*num))\n",
    "    \n",
    "ap_10 = ap_10 / (total_usr)\n",
    "\n",
    "print (\"Over all average ap: %f\" % (ap_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_5 = 0\n",
    "\n",
    "for i in range(NUM_CLUSTER):\n",
    "    num = label_map[i]\n",
    "    \n",
    "    recall_5 += test_rec_5[i] * num\n",
    "    print (\"Cluster %d, aps: %f, num: %d, weighted ap: %f\" % (i, test_rec_5[i], num, test_rec_5[i]*num))\n",
    "    \n",
    "recall_5 = recall_5 / (total_usr)\n",
    "\n",
    "print (\"Over all average ap: %f\" % (recall_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_10 = 0\n",
    "\n",
    "for i in range(NUM_CLUSTER):\n",
    "    num = label_map[i]\n",
    "    \n",
    "    recall_10 += test_rec_10[i] * num\n",
    "    print (\"Cluster %d, aps: %f, num: %d, weighted ap: %f\" % (i, test_rec_10[i], num, test_rec_10[i]*num))\n",
    "    \n",
    "recall_10 = recall_10 / (total_usr)\n",
    "\n",
    "print (\"Over all average ap: %f\" % (recall_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For netflix huge dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "user_train_rating = np.load('../data/netflix/rating_matrix_CDAE.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_rating_all, train_indices_all, test_indices_all = gen_train_test(user_train_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "np.save('../data/netflix/train_rating_all.npy', train_rating_all)\n",
    "\n",
    "with open('../data/netflix/train_indices_all.pkl', 'wb') as train_indice_file:\n",
    "    pickle.dump(train_indices_all, train_indice_file)\n",
    "    \n",
    "with open('../data/netflix/test_indices_all.pkl', 'wb') as test_indice_file:\n",
    "    pickle.dump(test_indices_all, test_indice_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data = df.as_matrix()\n",
    "\n",
    "user_train_rating = np.zeros((total_usr, total_item), dtype=np.int32)\n",
    "\n",
    "for line in train_data:\n",
    "    uid = user_map[line[0]]\n",
    "    iid = item_map[line[1]]\n",
    "    user_train_rating[uid, iid] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_rating_all, train_indices_all, test_indices_all = gen_train_test(user_train_rating)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_user_all = np.nonzero(np.count_nonzero(train_rating_all, axis=1))[0]\n",
    "\n",
    "autoencoder = AutoEncoder(user_num=total_usr, item_num=total_item, mode='user', loss_function='log_loss',\n",
    "                          batch_size=100, epochs=1000)\n",
    "\n",
    "\n",
    "autoencoder.train(rating=train_rating_all,\n",
    "                  train_idents=train_user_all,\n",
    "                  train_indices=train_indices_all,\n",
    "                  test_indices=test_indices_all)\n",
    "\n",
    "autoencoder.model_save(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loss = autoencoder.log['train_loss']\n",
    "\n",
    "plt.plot(range(len(train_loss)), train_loss, color='blue', label='Train loss')\n",
    "# plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('#Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ap = autoencoder.log['ap@5']\n",
    "\n",
    "top_N_ap = sorted(test_ap,reverse=True)[:20]\n",
    "\n",
    "plt.plot(range(5, (len(test_ap)+1)*5, 5), test_ap, color='green', label='Test AP')\n",
    "# plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Train 200 epoch\")\n",
    "plt.xlabel('#Epoch')\n",
    "plt.ylabel('AP')\n",
    "plt.show()\n",
    "\n",
    "# print (np.mean(top_N_ap))\n",
    "\n",
    "print (max(test_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_recall = autoencoder.log['recall@10']\n",
    "\n",
    "plt.plot(range(1000, (len(test_recall)+1)*1000, 1000), test_recall, color='green', label='Test Recall')\n",
    "# plt.plot(range(len(test_loss)), test_loss, color='red', label='Test loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Train 200 epoch\")\n",
    "plt.xlabel('#Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.show()\n",
    "\n",
    "# print (np.mean(top_N_ap))\n",
    "\n",
    "print (max(test_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "autoencoder = AutoEncoder(user_num=total_usr, item_num=total_item, mode='user', loss_function='cross_entropy',\n",
    "                          batch_size=100, epochs=1000)\n",
    "autoencoder.model_load(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_onehot_vectors = autoencoder.sess.run(autoencoder.vector_matrix)\n",
    "u_vectors = np.load('../data/netflix/user_vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors = np.zeros((total_usr, 20), dtype=np.float32)\n",
    "for i in range(total_usr):\n",
    "    feature_vectors[i] = autoencoder.sess.run(\n",
    "        autoencoder.code,\n",
    "        feed_dict={\n",
    "            autoencoder.input: [train_rating_all[i]],\n",
    "            autoencoder.ident: [i]\n",
    "        })\n",
    "    \n",
    "np.save('../data/netflix/feature_vectors.npy', feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_onehot_vectors = autoencoder.sess.run(autoencoder.vector_matrix)\n",
    "\n",
    "np.save('../data/netflix/user_vectors.npy', user_onehot_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
